{
    "ab473113-4ab6-4d35-b791-0095d6b7a77c": {
        "pdf_name": "305 position paper.pdf",
        "pdf_text": "Haamed Rahman \nProfessor Mei \nCS 305 \n10/29/2025\u200b  \nThe Balance between AI and Learning \nAs AI tools like ChatGPT become increasingly accessible, a common concern is \ndrawing the line between using the technology as a learning enhancement versus a \nhindrance to our education. Though this depends on how each student uses the \ntechnology, policies often lack the nuance to make that distinction. The issue becomes \nmore pronounced in writing courses, where the use of AI can mimic tools such as \nGrammarly, offering feedback and clarification, while also raising the risk that these \ntools might replace the line of reasoning expected to be developed through the writing \nprocess. Because of this, my position is that students may only be permitted to leverage \nGenAI tools for final-stage feedback, surface-level editing, and final review before \nsubmission. It should be noted, however, that the usage of AI for brainstorming, \noutlining, drafting, and early revising should remain prohibited. This distinction is \nessential to preserve the cognitive struggle necessary for the proper understanding of \nthe course's target. This position further aligns with university policies, which prohibit \nGenAI from \u201ccompleting academic work\u201d, as well as findings that we fail to develop \nproper critical thinking when we use AI to draft and rewrite our papers[2]. Furthermore, \nstudents who rely heavily on AI tend to display \u201clower cognitive effort\u201d and \u201cweaker \nlong-term mastery\u201d in course materials[2]. If the purpose of writing in this course is to \ntrain our analytical and communication skills, then allowing AI to interfere in the \nfoundational processes compromises that goal. Therefore, our policy is clear: GenAI \nmay be used as a final level of review to polish human work and ensure it aligns with \nthe objectives, but cannot be a substitute for the human effort. \nThe fundamental issue with allowing students to use GenAI in any early stage of \nwriting, such as brainstorming, outlining, or drafting, is that it eliminates the cognitive \nstruggle that is essential to meaningful learning[2]. Writing isn\u2019t simply about arranging \nsentences in a grammatically correct manner, it's about organizing one's thoughts, \ndeveloping their reasoning, and understanding the course material thoroughly. When AI \ndrafts, outlines, and writes a response, it not only prevents the student from engaging \nwith the material, but it also fails to understand the course material itself, it merely takes \na statistical guess on what is probably the correct arrangement. This aligns with the \nposition that we bypass the learning objectives of the course when we allow AI to \nperform these foundational tasks [1]. Moreover, cognitive research shows that learning \noccurs most effectively when students engage in thoughtful reasoning and problem \nsolving, especially when the process is difficult and slow [2]. When AI tools share this \nload, they reduce the long-term retention and understanding that the student could aim \nto achieve. Across studies, students have shown that when they depend on AI tools, \n\nthey exert \u201clower cognitive load\u201d and consequently \u201cweaker long-term understanding\u201d of \ncourse materials[2]. Although the final output may be grammatically correct, neither the \nstudent nor the model truly understands what is being submitted, they merely take a risk \non the quality of the output. This ultimately compromises the fundamental purpose of \nour education. \nA common argument used to refute these positions is that AI simply extends the \neffectiveness of similar tools we allow in classrooms, such as Grammarly and built-in \nspell checkers. If we already allow students to fix grammar and writing with these tools, \nwhy can we not leverage AI to improve style, format, and clarity? The difference comes \nwhen these AI tools generate outputs that go beyond simple grammar and \nsuggestions(often even when the prompt explicitly instructs against it), and generate \ntexts with unnoticed revisions, improvements, and restructuring. The ease with which \nthis process can happen(sometimes unnoticeably) denies the student the opportunity to \nunderstand the flaws with their initial implementation and the suggested response. \nUnlike current tools that merely highlight the issues, AI offers large-scale revisions by \ngenerating completely new outputs, without highlighting the flaws. This is a crucial \ndistinction. While spellcheck tools are limited in nature, AI outputs can easily become \nentirely new papers, and because it is often the case that students view AI outputs as \ninfallible, they accept these outputs without analyzing or even understanding them. \nBecause of this attitude, students hand off their critical thinking to AI tools, an algorithm \nthat does not understand the assignment goals, nor the student\u2019s position. Even when a \nstudent's intention is not to cheat, AI\u2019s polished outputs become extremely attractive in \ncomparison to a student's own output, leading students to begin accepting these edits \nwithout analysis, leading to \u201cmetacognitive laziness\u201d[2]. Ultimately, it becomes \nincreasingly easy to offload these tasks to AI tools rather than work through the difficulty \nof understanding the course materials. \n\u200b\nIn order to protect our course objectives, we must adopt a balanced policy, \ndistinguishing between permission and productive AI usage, and actions that \ncompromise our objectives. Therefore, students should only be permitted to use AI for \nthe final stage review, such as looking over grammar, clarity, and tone, once a student's \nown ideas are fully developed, much like how a teacher would give feedback. Initial \nstages, such as drafting and outlining with AI, should remain prohibited as these stages \nare crucial for developing critical thinking skills and material comprehension. If AI makes \ncognitive effort optional, then classrooms must enforce that effort through their structure \nand intentions [1]. To enforce this, instructors can encourage and grade on the writing \nprocess, such as deadlines for drafts and each stage. The future of education depends \non our ability to integrate our tools responsibly, not abuse them for short-term gains, or \nshun them and stunt our growth. If our writing courses aim to build critical thinking skills, \nthen students must be required to do the difficult parts themselves. AI tools may \n\nproduce polished final products, but they cannot replicate the thoughtfulness and \nintention that goes into a product built through struggle and determination. \n \n \n[1] C. Shirky. \u201cStudents Hate Them. Universities Need Them. The Only Real Solution to the A.I. \nCheating Crisis.\u201d *The New York Times*, Aug. 26, 2025. \n[2] B. W. Stone. \u201cHow Does AI Affect How We Learn? A Cognitive Psychologist Explains Why \nYou Learn When the Work Is Hard.\u201d *The Conversation*, Sep. 10, 2025. \n \n \n \n \n \n\n",
        "created_at": "2025-11-08T12:40:40.221390",
        "messages": [
            {
                "role": "assistant",
                "content": "The document by Haamed Rahman discusses the role of AI tools, like ChatGPT, in education, particularly in writing courses. Rahman argues that while AI can enhance learning by providing feedback and editing assistance, it poses a risk when used in early stages of writing, such as brainstorming and drafting. This is because it can replace the cognitive struggle necessary for developing critical thinking and understanding course materials. The document suggests that AI should be used only for final-stage review, similar to a teacher's feedback, to polish grammar, clarity, and tone after a student's ideas are fully developed. Rahman's position aligns with university policies against AI completing academic work and emphasizes the importance of maintaining the integrity of the learning process. The document also addresses the misconception that AI tools are merely advanced versions of existing tools like Grammarly, highlighting that AI can generate entirely new outputs, potentially leading to \"metacognitive laziness.\" To preserve educational objectives, Rahman advocates for policies that balance AI use by enforcing cognitive effort and critical thinking",
                "timestamp": "2025-11-08T12:40:40.221390"
            },
            {
                "role": "user",
                "content": "So basically ai usage in early stages can stunt long term growth?",
                "timestamp": "2025-11-08T12:41:15.857793"
            },
            {
                "role": "assistant",
                "content": "Yes, according to the document by Haamed Rahman, the use of AI in the early stages of writing, such as brainstorming, outlining, and drafting, can indeed stunt long-term growth. Rahman argues that these early stages are crucial for developing critical thinking skills and a deep understanding of course materials. When AI is used for these foundational tasks, it eliminates the cognitive struggle necessary for meaningful learning, which can result in \"lower cognitive effort\" and \"weaker long-term mastery\" of the material. The document emphasizes that while AI can be beneficial for final-stage review, its use should be limited to ensure students engage with the challenging aspects of learning.",
                "timestamp": "2025-11-08T12:41:15.857793"
            }
        ]
    },
    "3f04d650-6b16-400e-9c65-5ce5cc7af398": {
        "pdf_name": "305 position paper.pdf",
        "pdf_text": "Haamed Rahman \nProfessor Mei \nCS 305 \n10/29/2025\u200b  \nThe Balance between AI and Learning \nAs AI tools like ChatGPT become increasingly accessible, a common concern is \ndrawing the line between using the technology as a learning enhancement versus a \nhindrance to our education. Though this depends on how each student uses the \ntechnology, policies often lack the nuance to make that distinction. The issue becomes \nmore pronounced in writing courses, where the use of AI can mimic tools such as \nGrammarly, offering feedback and clarification, while also raising the risk that these \ntools might replace the line of reasoning expected to be developed through the writing \nprocess. Because of this, my position is that students may only be permitted to leverage \nGenAI tools for final-stage feedback, surface-level editing, and final review before \nsubmission. It should be noted, however, that the usage of AI for brainstorming, \noutlining, drafting, and early revising should remain prohibited. This distinction is \nessential to preserve the cognitive struggle necessary for the proper understanding of \nthe course's target. This position further aligns with university policies, which prohibit \nGenAI from \u201ccompleting academic work\u201d, as well as findings that we fail to develop \nproper critical thinking when we use AI to draft and rewrite our papers[2]. Furthermore, \nstudents who rely heavily on AI tend to display \u201clower cognitive effort\u201d and \u201cweaker \nlong-term mastery\u201d in course materials[2]. If the purpose of writing in this course is to \ntrain our analytical and communication skills, then allowing AI to interfere in the \nfoundational processes compromises that goal. Therefore, our policy is clear: GenAI \nmay be used as a final level of review to polish human work and ensure it aligns with \nthe objectives, but cannot be a substitute for the human effort. \nThe fundamental issue with allowing students to use GenAI in any early stage of \nwriting, such as brainstorming, outlining, or drafting, is that it eliminates the cognitive \nstruggle that is essential to meaningful learning[2]. Writing isn\u2019t simply about arranging \nsentences in a grammatically correct manner, it's about organizing one's thoughts, \ndeveloping their reasoning, and understanding the course material thoroughly. When AI \ndrafts, outlines, and writes a response, it not only prevents the student from engaging \nwith the material, but it also fails to understand the course material itself, it merely takes \na statistical guess on what is probably the correct arrangement. This aligns with the \nposition that we bypass the learning objectives of the course when we allow AI to \nperform these foundational tasks [1]. Moreover, cognitive research shows that learning \noccurs most effectively when students engage in thoughtful reasoning and problem \nsolving, especially when the process is difficult and slow [2]. When AI tools share this \nload, they reduce the long-term retention and understanding that the student could aim \nto achieve. Across studies, students have shown that when they depend on AI tools, \n\nthey exert \u201clower cognitive load\u201d and consequently \u201cweaker long-term understanding\u201d of \ncourse materials[2]. Although the final output may be grammatically correct, neither the \nstudent nor the model truly understands what is being submitted, they merely take a risk \non the quality of the output. This ultimately compromises the fundamental purpose of \nour education. \nA common argument used to refute these positions is that AI simply extends the \neffectiveness of similar tools we allow in classrooms, such as Grammarly and built-in \nspell checkers. If we already allow students to fix grammar and writing with these tools, \nwhy can we not leverage AI to improve style, format, and clarity? The difference comes \nwhen these AI tools generate outputs that go beyond simple grammar and \nsuggestions(often even when the prompt explicitly instructs against it), and generate \ntexts with unnoticed revisions, improvements, and restructuring. The ease with which \nthis process can happen(sometimes unnoticeably) denies the student the opportunity to \nunderstand the flaws with their initial implementation and the suggested response. \nUnlike current tools that merely highlight the issues, AI offers large-scale revisions by \ngenerating completely new outputs, without highlighting the flaws. This is a crucial \ndistinction. While spellcheck tools are limited in nature, AI outputs can easily become \nentirely new papers, and because it is often the case that students view AI outputs as \ninfallible, they accept these outputs without analyzing or even understanding them. \nBecause of this attitude, students hand off their critical thinking to AI tools, an algorithm \nthat does not understand the assignment goals, nor the student\u2019s position. Even when a \nstudent's intention is not to cheat, AI\u2019s polished outputs become extremely attractive in \ncomparison to a student's own output, leading students to begin accepting these edits \nwithout analysis, leading to \u201cmetacognitive laziness\u201d[2]. Ultimately, it becomes \nincreasingly easy to offload these tasks to AI tools rather than work through the difficulty \nof understanding the course materials. \n\u200b\nIn order to protect our course objectives, we must adopt a balanced policy, \ndistinguishing between permission and productive AI usage, and actions that \ncompromise our objectives. Therefore, students should only be permitted to use AI for \nthe final stage review, such as looking over grammar, clarity, and tone, once a student's \nown ideas are fully developed, much like how a teacher would give feedback. Initial \nstages, such as drafting and outlining with AI, should remain prohibited as these stages \nare crucial for developing critical thinking skills and material comprehension. If AI makes \ncognitive effort optional, then classrooms must enforce that effort through their structure \nand intentions [1]. To enforce this, instructors can encourage and grade on the writing \nprocess, such as deadlines for drafts and each stage. The future of education depends \non our ability to integrate our tools responsibly, not abuse them for short-term gains, or \nshun them and stunt our growth. If our writing courses aim to build critical thinking skills, \nthen students must be required to do the difficult parts themselves. AI tools may \n\nproduce polished final products, but they cannot replicate the thoughtfulness and \nintention that goes into a product built through struggle and determination. \n \n \n[1] C. Shirky. \u201cStudents Hate Them. Universities Need Them. The Only Real Solution to the A.I. \nCheating Crisis.\u201d *The New York Times*, Aug. 26, 2025. \n[2] B. W. Stone. \u201cHow Does AI Affect How We Learn? A Cognitive Psychologist Explains Why \nYou Learn When the Work Is Hard.\u201d *The Conversation*, Sep. 10, 2025. \n \n \n \n \n \n\n",
        "created_at": "2025-11-08T12:43:39.634040",
        "messages": [
            {
                "role": "assistant",
                "content": "The document discusses the appropriate use of AI tools like ChatGPT in educational contexts, particularly in writing courses. The author, Haamed Rahman, argues that while AI can enhance learning by providing feedback and editing, it should not replace the cognitive processes involved in brainstorming, outlining, drafting, and revising. The document emphasizes that using AI in early stages of writing can hinder the development of critical thinking and understanding of course material. Rahman suggests that AI should be limited to final-stage reviews to polish students' work, aligning with university policies that prohibit AI from completing academic tasks. The document highlights the risk of students becoming overly reliant on AI, leading to \"metacognitive laziness\" and weaker long-term mastery. To maintain educational objectives, the author advocates for a balanced policy that allows AI use only for final reviews and encourages instructors to focus on the writing process.",
                "timestamp": "2025-11-08T12:43:39.634040"
            },
            {
                "role": "user",
                "content": "so no ai",
                "timestamp": "2025-11-08T12:43:47.824639"
            },
            {
                "role": "assistant",
                "content": "Based on the document, AI tools like ChatGPT should not be used in the early stages of writing, such as brainstorming, outlining, and drafting. These tasks are crucial for developing critical thinking skills and understanding course material. AI is only recommended for use in the final-stage review to polish work by checking grammar, clarity, and tone. This policy helps ensure that students engage in meaningful learning and develop the necessary cognitive skills.",
                "timestamp": "2025-11-08T12:43:47.824639"
            },
            {
                "role": "user",
                "content": "it makes us stupid?",
                "timestamp": "2025-11-08T12:43:58.566357"
            },
            {
                "role": "assistant",
                "content": "The document suggests that using AI tools like ChatGPT inappropriately, particularly in the early stages of writing, can hinder learning by reducing cognitive effort and critical thinking. This can lead to \"metacognitive laziness\" and weaker long-term understanding of course materials. The author argues that while AI can be beneficial for final-stage reviews, relying on it for foundational tasks like brainstorming and drafting compromises educational objectives. Therefore, it's not that AI inherently makes us \"stupid,\" but that misuse can impede the learning process and development of important skills.",
                "timestamp": "2025-11-08T12:43:58.566357"
            }
        ]
    }
}